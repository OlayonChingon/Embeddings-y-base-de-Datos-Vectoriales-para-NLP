{"cells":[{"cell_type":"markdown","source":["# Cálculo de distancias de vectores con NumPy"],"metadata":{"id":"cWRfYFSJPWOY"}},{"cell_type":"markdown","source":["## Importar NumPy y cargar vectores"],"metadata":{"id":"60-iW1o6PM2D"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["import numpy as np"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"I0BwH8n4O0lV"}},{"cell_type":"markdown","source":["La contruccion de los vectores esta echa convenientemente para las similitudes semanticas tengan sentido. ES UN EJEMPLO TEORICO"],"metadata":{"id":"iqwzlCPKbLrI"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["gato = np.array([0.9, 0.8, 0.2])\n","perro = np.array([0.8, 0.85, 0.15])\n","computadora = np.array([0.1, 0.3, 0.9])"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"E3rSz1hGO0lW"}},{"cell_type":"markdown","source":["## Dot product\n","\n","El producto punto (dot product) entre los vectores \"gato\" y \"perro\" se puede calcular utilizando la operación de producto punto en Python. np.dot"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"rYhTz4q8O0lX"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Producto punto gato-perro:  1.4300000000000002\n","Producto punto gato-computadora:  0.51\n"]}],"source":["dot_product_gato_perro = np.dot(gato,perro)\n","dot_product_gato_computadora = np.dot(gato, computadora)\n","\n","print('Producto punto gato-perro: ', dot_product_gato_perro )\n","print('Producto punto gato-computadora: ',dot_product_gato_computadora )\n"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"QVWTk69SO0lY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690912944028,"user_tz":300,"elapsed":3,"user":{"displayName":"Carlos Alarcón","userId":"15349885202925550395"}},"outputId":"1bac75f4-d530-4cb5-e62f-76091eb13aee"}},{"cell_type":"markdown","source":["## Euclidean distance\n","\n"," la distancia euclidiana entre los vectores \"gato\" y \"perro\" se puede calcular utilizando la función np.linalg.norm de la biblioteca numpy en Python.\n","\n"," La distancia euclidiana mide la longitud del vector resultante de la diferencia entre los dos vectores.\n","\n"," El resultado de la distancia euclidiana será un valor numérico que indica la longitud o magnitud de la diferencia entre los vectores \"gato\" y \"perro\". Un valor más bajo indica una menor distancia o similitud, mientras que un valor más alto indica una mayor distancia o disimilitud."],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"ozM8DAc3O0lZ"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.12247448713915887\n"]}],"source":["euclidean_distance = np.linalg.norm(gato-perro)\n","\n","print(euclidean_distance)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"YpHLRl4xO0lZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690912976654,"user_tz":300,"elapsed":294,"user":{"displayName":"Carlos Alarcón","userId":"15349885202925550395"}},"outputId":"4c4f7e25-6edc-4f5b-af32-af515fcfbbda"}},{"cell_type":"code","source":["euclidean_distance = np.linalg.norm(gato-computadora)\n","\n","print(euclidean_distance)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tt2ttkF1RHww","executionInfo":{"status":"ok","timestamp":1690912988508,"user_tz":300,"elapsed":3,"user":{"displayName":"Carlos Alarcón","userId":"15349885202925550395"}},"outputId":"b5e59018-2f83-4b14-b724-99383ce00556"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.174734012447073\n"]}]},{"cell_type":"markdown","source":["## Cosine similarity\n","\n"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"PUCbNs9cO0la"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9954467122628464\n"]}],"source":["dot_product = np.dot(gato, perro)\n","\n","norm_gato = np.linalg.norm(gato)\n","norm_perro = np.linalg.norm(perro)\n","\n","cosine_similarity = dot_product / (norm_gato * norm_perro)\n","print(cosine_similarity)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"QuWfDgyMO0la","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690913051208,"user_tz":300,"elapsed":2,"user":{"displayName":"Carlos Alarcón","userId":"15349885202925550395"}},"outputId":"72b19be3-92bd-48aa-fd62-63ebb353a12e"}},{"cell_type":"code","source":["1-cosine_similarity"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-g-z856RbAW","executionInfo":{"status":"ok","timestamp":1690913066286,"user_tz":300,"elapsed":212,"user":{"displayName":"Carlos Alarcón","userId":"15349885202925550395"}},"outputId":"0015650f-0473-4cb7-8b95-a1ea17f2a04d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.004553287737153577"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Intento vectorizar palabras utilizando el modulo Sentencetransformers sin buenos resultados en la similitudes"],"metadata":{"id":"1YltWpXfXjW_"}},{"cell_type":"code","source":["!pip install -U sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20AAWr35Zhc_","executionInfo":{"status":"ok","timestamp":1710661987277,"user_tz":180,"elapsed":8069,"user":{"displayName":"Fabian villada","userId":"07381010245664383048"}},"outputId":"e73ae68b-58c1-4eb7-cff4-336cbaa1fd87"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.99)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"]}]},{"cell_type":"code","source":["\n","from sentence_transformers import SentenceTransformer, util     #importa dos módulos de la biblioteca \"sentence-transformers\" en Python: \"SentenceTransformer\" y \"util\".\n","                                                                #La biblioteca \"util\" en la biblioteca \"sentence-transformers\" proporciona varias funciones adicionales y utilidades que son útiles\n","                                                                #para trabajar con embeddings de oraciones\n","\n","model = SentenceTransformer('all-MiniLM-L6-v2')                 #Podemos crear un objeto \"SentenceTransformer\" pasándole el nombre o la ruta del modelo pre-entrenado que deseas utilizar.\n","                                                                #En este caso una instancia del modelo pre-entrenado llamado \"all-MiniLM-L6-v2\" de la biblioteca \"sentence-transformers\".\n","                                                                #En \"sentence-transformers\" podemos utilizar modelos espesificos para ciertas tareas.\n","                                                                #el modelo \"all-MiniLM-L6-v2\" en la biblioteca \"sentence-transformers\" es útil para generar embeddings de oraciones y\n","                                                                #utilizarlos en una variedad de tareas de procesamiento de lenguaje natural, incluyendo búsqueda semántica, clasificación de textos,\n","                                                                #generación de paráfrasis y transferencia de aprendizaje.\n","\n","#Lista de frases\n","sentences = [\"gato\",\n","             'perro',\n","             'computadora',\n","             \"Impresora\",\n","             'pasta',\n","             'pizza']\n","\n","#Cómputo de embeddings\n","embeddings = model.encode(sentences)\n"],"metadata":{"id":"Ur_N3MO4VImS","executionInfo":{"status":"ok","timestamp":1710662305923,"user_tz":180,"elapsed":2466,"user":{"displayName":"Fabian villada","userId":"07381010245664383048"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["embeddings  #Lista de vectores donde cada vector representa una de mis palabras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJLrrbnUWR_L","executionInfo":{"status":"ok","timestamp":1710662305924,"user_tz":180,"elapsed":11,"user":{"displayName":"Fabian villada","userId":"07381010245664383048"}},"outputId":"086ce948-c12d-4167-8cbf-615433cd77aa"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.10839419,  0.07542189, -0.08832745, ..., -0.0192022 ,\n","        -0.020419  ,  0.04669119],\n","       [-0.00161942, -0.00580623, -0.06966979, ..., -0.04907138,\n","        -0.00524239,  0.01036614],\n","       [-0.00765345, -0.00026108, -0.03310274, ...,  0.05503176,\n","         0.07862205, -0.04347958],\n","       [-0.0200004 , -0.02159091,  0.05513196, ..., -0.01734902,\n","         0.07802194,  0.00851926],\n","       [-0.07682363, -0.01107207, -0.02245897, ...,  0.05667981,\n","         0.09134056, -0.015685  ],\n","       [-0.08696938,  0.0699105 , -0.01509741, ...,  0.02898977,\n","         0.05629738, -0.03187963]], dtype=float32)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["\n","cosine_scores = util.cos_sim(embeddings,embeddings)                           #Se utiliza la función \"cos_sim\" del módulo \"util\" para calcular la similitud del coseno entre cada par de embeddings\n","                                                                              #de las frases en la variable embeddings (sentencias).Es decir evalua la cercania de los vectores.\n","                                                                              #La variable cosine_scores almacena una matriz de similitudes coseno, donde cosine_scores[i][j] representa\n","                                                                              #la similitud del coseno entre la frase i y la frase j.\n","\n","\n","#Encuentra las parejas con los puntajes de similitud del coseno más altos\n","pairs = []\n","\n","for i in range(len(cosine_scores)-1):                                         #se recorre la matriz de similitudes coseno y se almacenan las parejas de índices de frases junto con sus puntajes de similitud\n","    for j in range(i+1, len(cosine_scores)):                                  #en la lista pairs. El bucle externo itera desde el índice 0 hasta el penúltimo índice, y el bucle interno itera desde el índice\n","                                                                              #siguiente al índice externo hasta el último índice, asegurando que cada par de frases se considere una vez sin duplicados.\n","        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})           # En esta linea dentro del doble bucle for agregamos un diccionario a la lista pairs. Este diccionario contiene información sobre\n","                                                                              #una pareja de frases, incluyendo los índices de las frases en la matriz de similitud del coseno y su puntaje de similitud.\n","                                                                              #'index': Es la clave que se utiliza para almacenar los índices de las frases en la matriz de similitud del coseno.\n","                                                                              #'score': Es la clave que se utiliza para almacenar el puntaje de similitud del coseno entre las dos frases representadas por\n","                                                                              #los índices i y j.\n","                                                                              #Es decir tendriamos en index la sentecia 1 comparada con la dos y en score el valor de la similitud del coseno y\n","                                                                              #asi con 1 y 3, 1 y 4 ,...., 2 y 3 etc\n","\n","\n","#Ordena los puntajes en orden descendente.\n","pairs = sorted(pairs, key=lambda x : x['score'],reverse=True)                 #La función sorted() devuelve una nueva lista que contiene los elementos de pairs ordenados según el valor del puntaje de\n","                                                                              #similitud del coseno en orden descendente.\n","                                                                              #Esto significa que los primeros elementos de la lista pairs ahora tendrán los puntajes de similitud del coseno más altos,\n","                                                                              #mientras que los últimos elementos tendrán los puntajes más bajos.\n","\n","for pair in pairs[0:5]:\n","    i, j = pair['index']\n","    print(\"{} ----> {}\".format(sentences[i], sentences[j]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZzKI0_cW3nG","executionInfo":{"status":"ok","timestamp":1710662308865,"user_tz":180,"elapsed":420,"user":{"displayName":"Fabian villada","userId":"07381010245664383048"}},"outputId":"5f5fc350-f160-4489-d920-d34fac2a7558"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["pasta ----> pizza\n","perro ----> Impresora\n","computadora ----> Impresora\n","gato ----> pasta\n","computadora ----> pasta\n"]}]},{"cell_type":"code","source":["pairs"],"metadata":{"id":"-_0YNFSdXBSV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Como comparacion SentenceTransformer con oraciones"],"metadata":{"id":"BAB5Jhelet3Z"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util     #importa dos módulos de la biblioteca \"sentence-transformers\" en Python: \"SentenceTransformer\" y \"util\".\n","                                                                #La biblioteca \"util\" en la biblioteca \"sentence-transformers\" proporciona varias funciones adicionales y utilidades que son útiles\n","                                                                #para trabajar con embeddings de oraciones\n","\n","model = SentenceTransformer('all-MiniLM-L6-v2')                 #Podemos crear un objeto \"SentenceTransformer\" pasándole el nombre o la ruta del modelo pre-entrenado que deseas utilizar.\n","                                                                #En este caso una instancia del modelo pre-entrenado llamado \"all-MiniLM-L6-v2\" de la biblioteca \"sentence-transformers\".\n","                                                                #En \"sentence-transformers\" podemos utilizar modelos espesificos para ciertas tareas.\n","                                                                #el modelo \"all-MiniLM-L6-v2\" en la biblioteca \"sentence-transformers\" es útil para generar embeddings de oraciones y\n","                                                                #utilizarlos en una variedad de tareas de procesamiento de lenguaje natural, incluyendo búsqueda semántica, clasificación de textos,\n","                                                                #generación de paráfrasis y transferencia de aprendizaje.\n","\n","#Lista de frases\n","sentences = [\"Como gato y raton\",\n","             'Mi perro se llama pluto',\n","             'La computadora no enciende',\n","             'Se corto internet en casa ',\n","             'python es un buen leguaje de programacion de computadoras',\n","             'Hoy comere Pastapasta',\n","             'La pizza esta lista',\n","             'Hoy cenere en un restaurant']\n","\n","#Cómputo de embeddings\n","embeddings = model.encode(sentences)\n","\n","cosine_scores = util.cos_sim(embeddings,embeddings)                           #Se utiliza la función \"cos_sim\" del módulo \"util\" para calcular la similitud del coseno entre cada par de embeddings\n","                                                                              #de las frases en la variable embeddings (sentencias).Es decir evalua la cercania de los vectores.\n","                                                                              #La variable cosine_scores almacena una matriz de similitudes coseno, donde cosine_scores[i][j] representa\n","                                                                              #la similitud del coseno entre la frase i y la frase j.\n","\n","\n","#Encuentra las parejas con los puntajes de similitud del coseno más altos\n","pairs = []\n","\n","for i in range(len(cosine_scores)-1):                                         #se recorre la matriz de similitudes coseno y se almacenan las parejas de índices de frases junto con sus puntajes de similitud\n","    for j in range(i+1, len(cosine_scores)):                                  #en la lista pairs. El bucle externo itera desde el índice 0 hasta el penúltimo índice, y el bucle interno itera desde el índice\n","                                                                              #siguiente al índice externo hasta el último índice, asegurando que cada par de frases se considere una vez sin duplicados.\n","        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})           # En esta linea dentro del doble bucle for agregamos un diccionario a la lista pairs. Este diccionario contiene información sobre\n","                                                                              #una pareja de frases, incluyendo los índices de las frases en la matriz de similitud del coseno y su puntaje de similitud.\n","                                                                              #'index': Es la clave que se utiliza para almacenar los índices de las frases en la matriz de similitud del coseno.\n","                                                                              #'score': Es la clave que se utiliza para almacenar el puntaje de similitud del coseno entre las dos frases representadas por\n","                                                                              #los índices i y j.\n","                                                                              #Es decir tendriamos en index la sentecia 1 comparada con la dos y en score el valor de la similitud del coseno y\n","                                                                              #asi con 1 y 3, 1 y 4 ,...., 2 y 3 etc\n","\n","\n","#Ordena los puntajes en orden descendente.\n","pairs = sorted(pairs, key=lambda x : x['score'],reverse=True)                 #La función sorted() devuelve una nueva lista que contiene los elementos de pairs ordenados según el valor del puntaje de\n","                                                                              #similitud del coseno en orden descendente.\n","                                                                              #Esto significa que los primeros elementos de la lista pairs ahora tendrán los puntajes de similitud del coseno más altos,\n","                                                                              #mientras que los últimos elementos tendrán los puntajes más bajos.\n","\n","for pair in pairs[0:8]:\n","    i, j = pair['index']\n","    print(\"{} ----> {}\".format(sentences[i], sentences[j]))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NklE3BJyX6BH","executionInfo":{"status":"ok","timestamp":1710661854589,"user_tz":180,"elapsed":2995,"user":{"displayName":"Fabian villada","userId":"07381010245664383048"}},"outputId":"b244388c-fd6f-42af-a027-75a0c74766e1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Hoy comere Pastapasta ----> Hoy cenere en un restaurant\n","La computadora no enciende ----> Se corto internet en casa \n","La pizza esta lista ----> Hoy cenere en un restaurant\n","Hoy comere Pastapasta ----> La pizza esta lista\n","Como gato y raton ----> La computadora no enciende\n","Se corto internet en casa  ----> Hoy cenere en un restaurant\n","Mi perro se llama pluto ----> La pizza esta lista\n","Como gato y raton ----> Mi perro se llama pluto\n"]}]},{"cell_type":"code","source":["pairs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQEQmN2lZPux","executionInfo":{"status":"ok","timestamp":1710661900661,"user_tz":180,"elapsed":543,"user":{"displayName":"Fabian villada","userId":"07381010245664383048"}},"outputId":"ab9d3049-674d-425e-98fd-9c5315aac386"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'index': [5, 7], 'score': tensor(0.5596)},\n"," {'index': [2, 3], 'score': tensor(0.4973)},\n"," {'index': [6, 7], 'score': tensor(0.4973)},\n"," {'index': [5, 6], 'score': tensor(0.4681)},\n"," {'index': [0, 2], 'score': tensor(0.4362)},\n"," {'index': [3, 7], 'score': tensor(0.4358)},\n"," {'index': [1, 6], 'score': tensor(0.4281)},\n"," {'index': [0, 1], 'score': tensor(0.4272)},\n"," {'index': [2, 4], 'score': tensor(0.4192)},\n"," {'index': [1, 3], 'score': tensor(0.4149)},\n"," {'index': [3, 6], 'score': tensor(0.4141)},\n"," {'index': [2, 7], 'score': tensor(0.4101)},\n"," {'index': [0, 3], 'score': tensor(0.4031)},\n"," {'index': [2, 6], 'score': tensor(0.3998)},\n"," {'index': [1, 2], 'score': tensor(0.3935)},\n"," {'index': [2, 5], 'score': tensor(0.3557)},\n"," {'index': [0, 5], 'score': tensor(0.3451)},\n"," {'index': [0, 6], 'score': tensor(0.3396)},\n"," {'index': [1, 5], 'score': tensor(0.3343)},\n"," {'index': [3, 5], 'score': tensor(0.3105)},\n"," {'index': [0, 7], 'score': tensor(0.3051)},\n"," {'index': [1, 7], 'score': tensor(0.2892)},\n"," {'index': [3, 4], 'score': tensor(0.2164)},\n"," {'index': [4, 7], 'score': tensor(0.1933)},\n"," {'index': [4, 5], 'score': tensor(0.1930)},\n"," {'index': [0, 4], 'score': tensor(0.1894)},\n"," {'index': [1, 4], 'score': tensor(0.0896)},\n"," {'index': [4, 6], 'score': tensor(0.0819)}]"]},"metadata":{},"execution_count":7}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}